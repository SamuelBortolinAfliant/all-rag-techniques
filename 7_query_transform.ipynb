{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "source": [
    "# Query Transformations for Enhanced RAG Systems\n",
    "\n",
    "This notebook implements three query transformation techniques to enhance retrieval performance in RAG systems without relying on specialized libraries like LangChain. By modifying user queries, we can significantly improve the relevance and comprehensiveness of retrieved information.\n",
    "\n",
    "## Key Transformation Techniques\n",
    "\n",
    "1. **Query Rewriting**: Makes queries more specific and detailed for better search precision.\n",
    "2. **Step-back Prompting**: Generates broader queries to retrieve useful contextual information.\n",
    "3. **Sub-query Decomposition**: Breaks complex queries into simpler components for comprehensive retrieval."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up the Environment\n",
    "We begin by importing necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymupdf\n",
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up the OpenAI API Client\n",
    "We initialize the OpenAI client to generate embeddings and responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the OpenAI client with the base URL and API key\n",
    "client = OpenAI(\n",
    "    base_url=\"https://api.studio.nebius.com/v1/\",\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\")  # Retrieve the API key from environment variables\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing Query Transformation Techniques\n",
    "### 1. Query Rewriting\n",
    "This technique makes queries more specific and detailed to improve precision in retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rewrite_query(original_query, model=\"meta-llama/Llama-3.2-3B-Instruct\"):\n",
    "    \"\"\"\n",
    "    Rewrites a query to make it more specific and detailed for better retrieval.\n",
    "    \n",
    "    Args:\n",
    "        original_query (str): The original user query\n",
    "        model (str): The model to use for query rewriting\n",
    "        \n",
    "    Returns:\n",
    "        str: The rewritten query\n",
    "    \"\"\"\n",
    "    # Define the system prompt to guide the AI assistant's behavior\n",
    "    system_prompt = \"You are an AI assistant specialized in improving search queries. Your task is to rewrite user queries to be more specific, detailed, and likely to retrieve relevant information.\"\n",
    "    \n",
    "    # Define the user prompt with the original query to be rewritten\n",
    "    user_prompt = f\"\"\"\n",
    "    Rewrite the following query to make it more specific and detailed. Include relevant terms and concepts that might help in retrieving accurate information.\n",
    "    \n",
    "    Original query: {original_query}\n",
    "    \n",
    "    Rewritten query:\n",
    "    \"\"\"\n",
    "    \n",
    "    # Generate the rewritten query using the specified model\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        temperature=0.0,  # Low temperature for deterministic output\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # Return the rewritten query, stripping any leading/trailing whitespace\n",
    "    return response.choices[0].message.content.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Step-back Prompting\n",
    "This technique generates broader queries to retrieve contextual background information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_step_back_query(original_query, model=\"meta-llama/Llama-3.2-3B-Instruct\"):\n",
    "    \"\"\"\n",
    "    Generates a more general 'step-back' query to retrieve broader context.\n",
    "    \n",
    "    Args:\n",
    "        original_query (str): The original user query\n",
    "        model (str): The model to use for step-back query generation\n",
    "        \n",
    "    Returns:\n",
    "        str: The step-back query\n",
    "    \"\"\"\n",
    "    # Define the system prompt to guide the AI assistant's behavior\n",
    "    system_prompt = \"You are an AI assistant specialized in search strategies. Your task is to generate broader, more general versions of specific queries to retrieve relevant background information.\"\n",
    "    \n",
    "    # Define the user prompt with the original query to be generalized\n",
    "    user_prompt = f\"\"\"\n",
    "    Generate a broader, more general version of the following query that could help retrieve useful background information.\n",
    "    \n",
    "    Original query: {original_query}\n",
    "    \n",
    "    Step-back query:\n",
    "    \"\"\"\n",
    "    \n",
    "    # Generate the step-back query using the specified model\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        temperature=0.1,  # Slightly higher temperature for some variation\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # Return the step-back query, stripping any leading/trailing whitespace\n",
    "    return response.choices[0].message.content.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Sub-query Decomposition\n",
    "This technique breaks down complex queries into simpler components for comprehensive retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decompose_query(original_query, num_subqueries=4, model=\"meta-llama/Llama-3.2-3B-Instruct\"):\n",
    "    \"\"\"\n",
    "    Decomposes a complex query into simpler sub-queries.\n",
    "    \n",
    "    Args:\n",
    "        original_query (str): The original complex query\n",
    "        num_subqueries (int): Number of sub-queries to generate\n",
    "        model (str): The model to use for query decomposition\n",
    "        \n",
    "    Returns:\n",
    "        List[str]: A list of simpler sub-queries\n",
    "    \"\"\"\n",
    "    # Define the system prompt to guide the AI assistant's behavior\n",
    "    system_prompt = \"You are an AI assistant specialized in breaking down complex questions. Your task is to decompose complex queries into simpler sub-questions that, when answered together, address the original query.\"\n",
    "    \n",
    "    # Define the user prompt with the original query to be decomposed\n",
    "    user_prompt = f\"\"\"\n",
    "    Break down the following complex query into {num_subqueries} simpler sub-queries. Each sub-query should focus on a different aspect of the original question.\n",
    "    \n",
    "    Original query: {original_query}\n",
    "    \n",
    "    Generate {num_subqueries} sub-queries, one per line, in this format:\n",
    "    1. [First sub-query]\n",
    "    2. [Second sub-query]\n",
    "    And so on...\n",
    "    \"\"\"\n",
    "    \n",
    "    # Generate the sub-queries using the specified model\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        temperature=0.2,  # Slightly higher temperature for some variation\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # Process the response to extract sub-queries\n",
    "    content = response.choices[0].message.content.strip()\n",
    "    \n",
    "    # Extract numbered queries using simple parsing\n",
    "    lines = content.split(\"\\n\")\n",
    "    sub_queries = []\n",
    "    \n",
    "    for line in lines:\n",
    "        if line.strip() and any(line.strip().startswith(f\"{i}.\") for i in range(1, 10)):\n",
    "            # Remove the number and leading space\n",
    "            query = line.strip()\n",
    "            query = query[query.find(\".\")+1:].strip()\n",
    "            sub_queries.append(query)\n",
    "    \n",
    "    return sub_queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demonstrating Query Transformation Techniques\n",
    "Let's apply these techniques to an example query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Query: What are the impacts of AI on job automation and employment?\n",
      "\n",
      "1. Rewritten Query:\n",
      "Here's a rewritten query that's more specific and detailed:\n",
      "\n",
      "\"Examine the current trends and statistics on AI-driven job automation, including the types of jobs most susceptible to automation, the impact on employment rates, and the potential effects on the gig economy, as well as the role of education and retraining programs in mitigating the negative consequences of AI on the workforce.\"\n",
      "\n",
      "This rewritten query includes:\n",
      "\n",
      "1. Specific keywords: \"AI-driven job automation\", \"types of jobs most susceptible to automation\", and \"gig economy\" to help retrieve relevant information.\n",
      "2. Relevant concepts: \"current trends and statistics\", \"impact on employment rates\", and \"education and retraining programs\" to provide a more comprehensive understanding of the topic.\n",
      "3. Contextual phrases: \"mitigating the negative consequences\" to highlight the importance of addressing the potential negative effects of AI on employment.\n",
      "\n",
      "By incorporating these details, the rewritten query is more likely to retrieve accurate and relevant information on the topic.\n",
      "\n",
      "2. Step-back Query:\n",
      "Here's a broader, more general version of the original query:\n",
      "\n",
      "\"Effects of automation and artificial intelligence on the modern workforce, including job displacement, skill obsolescence, and the future of employment in various industries.\"\n",
      "\n",
      "This step-back query can help retrieve relevant background information on the topic, including:\n",
      "\n",
      "* The historical context of automation and AI on employment\n",
      "* The current state of job displacement and skill obsolescence\n",
      "* The impact of AI on different industries and sectors\n",
      "* The potential long-term effects on the global workforce\n",
      "* The role of education, training, and policy in mitigating the negative impacts of AI on employment\n",
      "\n",
      "3. Sub-queries:\n",
      "   1. What are the primary job roles that are most susceptible to automation by AI?\n",
      "   2. How do AI-powered automation systems currently affect employment rates and job creation in various industries?\n",
      "   3. What are the potential benefits of AI-driven job automation, such as increased productivity and efficiency?\n",
      "   4. How do governments, policymakers, and educators plan to address the challenges and consequences of AI-driven job automation on employment and the workforce?\n"
     ]
    }
   ],
   "source": [
    "# Example query\n",
    "original_query = \"What are the impacts of AI on job automation and employment?\"\n",
    "\n",
    "# Apply query transformations\n",
    "print(\"Original Query:\", original_query)\n",
    "\n",
    "# Query Rewriting\n",
    "rewritten_query = rewrite_query(original_query)\n",
    "print(\"\\n1. Rewritten Query:\")\n",
    "print(rewritten_query)\n",
    "\n",
    "# Step-back Prompting\n",
    "step_back_query = generate_step_back_query(original_query)\n",
    "print(\"\\n2. Step-back Query:\")\n",
    "print(step_back_query)\n",
    "\n",
    "# Sub-query Decomposition\n",
    "sub_queries = decompose_query(original_query, num_subqueries=4)\n",
    "print(\"\\n3. Sub-queries:\")\n",
    "for i, query in enumerate(sub_queries, 1):\n",
    "    print(f\"   {i}. {query}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Simple Vector Store\n",
    "To demonstrate how query transformations integrate with retrieval, let's implement a simple vector store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleVectorStore:\n",
    "    \"\"\"\n",
    "    A simple vector store implementation using NumPy.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initialize the vector store.\n",
    "        \"\"\"\n",
    "        self.vectors = []  # List to store embedding vectors\n",
    "        self.texts = []  # List to store original texts\n",
    "        self.metadata = []  # List to store metadata for each text\n",
    "    \n",
    "    def add_item(self, text, embedding, metadata=None):\n",
    "        \"\"\"\n",
    "        Add an item to the vector store.\n",
    "\n",
    "        Args:\n",
    "        text (str): The original text.\n",
    "        embedding (List[float]): The embedding vector.\n",
    "        metadata (dict, optional): Additional metadata.\n",
    "        \"\"\"\n",
    "        self.vectors.append(np.array(embedding))  # Convert embedding to numpy array and add to vectors list\n",
    "        self.texts.append(text)  # Add the original text to texts list\n",
    "        self.metadata.append(metadata or {})  # Add metadata to metadata list, use empty dict if None\n",
    "    \n",
    "    def similarity_search(self, query_embedding, k=5):\n",
    "        \"\"\"\n",
    "        Find the most similar items to a query embedding.\n",
    "\n",
    "        Args:\n",
    "        query_embedding (List[float]): Query embedding vector.\n",
    "        k (int): Number of results to return.\n",
    "\n",
    "        Returns:\n",
    "        List[Dict]: Top k most similar items with their texts and metadata.\n",
    "        \"\"\"\n",
    "        if not self.vectors:\n",
    "            return []  # Return empty list if no vectors are stored\n",
    "        \n",
    "        # Convert query embedding to numpy array\n",
    "        query_vector = np.array(query_embedding)\n",
    "        \n",
    "        # Calculate similarities using cosine similarity\n",
    "        similarities = []\n",
    "        for i, vector in enumerate(self.vectors):\n",
    "            # Compute cosine similarity between query vector and stored vector\n",
    "            similarity = np.dot(query_vector, vector) / (np.linalg.norm(query_vector) * np.linalg.norm(vector))\n",
    "            similarities.append((i, similarity))  # Append index and similarity score\n",
    "        \n",
    "        # Sort by similarity (descending)\n",
    "        similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # Return top k results\n",
    "        results = []\n",
    "        for i in range(min(k, len(similarities))):\n",
    "            idx, score = similarities[i]\n",
    "            results.append({\n",
    "                \"text\": self.texts[idx],  # Add the corresponding text\n",
    "                \"metadata\": self.metadata[idx],  # Add the corresponding metadata\n",
    "                \"similarity\": score  # Add the similarity score\n",
    "            })\n",
    "        \n",
    "        return results  # Return the list of top k similar items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embeddings(text, model=\"BAAI/bge-en-icl\"):\n",
    "    \"\"\"\n",
    "    Creates embeddings for the given text using the specified OpenAI model.\n",
    "\n",
    "    Args:\n",
    "    text (str): The input text for which embeddings are to be created.\n",
    "    model (str): The model to be used for creating embeddings.\n",
    "\n",
    "    Returns:\n",
    "    List[float]: The embedding vector.\n",
    "    \"\"\"\n",
    "    # Handle both string and list inputs by converting string input to a list\n",
    "    input_text = text if isinstance(text, list) else [text]\n",
    "    \n",
    "    # Create embeddings for the input text using the specified model\n",
    "    response = client.embeddings.create(\n",
    "        model=model,\n",
    "        input=input_text\n",
    "    )\n",
    "    \n",
    "    # If input was a string, return just the first embedding\n",
    "    if isinstance(text, str):\n",
    "        return response.data[0].embedding\n",
    "    \n",
    "    # Otherwise, return all embeddings as a list of vectors\n",
    "    return [item.embedding for item in response.data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing RAG with Query Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"\n",
    "    Extracts text from a PDF file.\n",
    "\n",
    "    Args:\n",
    "    pdf_path (str): Path to the PDF file.\n",
    "\n",
    "    Returns:\n",
    "    str: Extracted text from the PDF.\n",
    "    \"\"\"\n",
    "    # Open the PDF file\n",
    "    mypdf = pymupdf.open(pdf_path)\n",
    "    all_text = \"\"  # Initialize an empty string to store the extracted text\n",
    "\n",
    "    # Iterate through each page in the PDF\n",
    "    for page_num in range(mypdf.page_count):\n",
    "        page = mypdf[page_num]  # Get the page\n",
    "        text = page.get_text(\"text\")  # Extract text from the page\n",
    "        all_text += text  # Append the extracted text to the all_text string\n",
    "\n",
    "    return all_text  # Return the extracted text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text, n=1000, overlap=200):\n",
    "    \"\"\"\n",
    "    Chunks the given text into segments of n characters with overlap.\n",
    "\n",
    "    Args:\n",
    "    text (str): The text to be chunked.\n",
    "    n (int): The number of characters in each chunk.\n",
    "    overlap (int): The number of overlapping characters between chunks.\n",
    "\n",
    "    Returns:\n",
    "    List[str]: A list of text chunks.\n",
    "    \"\"\"\n",
    "    chunks = []  # Initialize an empty list to store the chunks\n",
    "    \n",
    "    # Loop through the text with a step size of (n - overlap)\n",
    "    for i in range(0, len(text), n - overlap):\n",
    "        # Append a chunk of text from index i to i + n to the chunks list\n",
    "        chunks.append(text[i:i + n])\n",
    "\n",
    "    return chunks  # Return the list of text chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_document(pdf_path, chunk_size=1000, chunk_overlap=200):\n",
    "    \"\"\"\n",
    "    Process a document for RAG.\n",
    "\n",
    "    Args:\n",
    "    pdf_path (str): Path to the PDF file.\n",
    "    chunk_size (int): Size of each chunk in characters.\n",
    "    chunk_overlap (int): Overlap between chunks in characters.\n",
    "\n",
    "    Returns:\n",
    "    SimpleVectorStore: A vector store containing document chunks and their embeddings.\n",
    "    \"\"\"\n",
    "    print(\"Extracting text from PDF...\")\n",
    "    extracted_text = extract_text_from_pdf(pdf_path)\n",
    "    \n",
    "    print(\"Chunking text...\")\n",
    "    chunks = chunk_text(extracted_text, chunk_size, chunk_overlap)\n",
    "    print(f\"Created {len(chunks)} text chunks\")\n",
    "    \n",
    "    print(\"Creating embeddings for chunks...\")\n",
    "    # Create embeddings for all chunks at once for efficiency\n",
    "    chunk_embeddings = create_embeddings(chunks)\n",
    "    \n",
    "    # Create vector store\n",
    "    store = SimpleVectorStore()\n",
    "    \n",
    "    # Add chunks to vector store\n",
    "    for i, (chunk, embedding) in enumerate(zip(chunks, chunk_embeddings)):\n",
    "        store.add_item(\n",
    "            text=chunk,\n",
    "            embedding=embedding,\n",
    "            metadata={\"index\": i, \"source\": pdf_path}\n",
    "        )\n",
    "    \n",
    "    print(f\"Added {len(chunks)} chunks to the vector store\")\n",
    "    return store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG with Query Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformed_search(query, vector_store, transformation_type, top_k=3):\n",
    "    \"\"\"\n",
    "    Search using a transformed query.\n",
    "    \n",
    "    Args:\n",
    "        query (str): Original query\n",
    "        vector_store (SimpleVectorStore): Vector store to search\n",
    "        transformation_type (str): Type of transformation ('rewrite', 'step_back', or 'decompose')\n",
    "        top_k (int): Number of results to return\n",
    "        \n",
    "    Returns:\n",
    "        List[Dict]: Search results\n",
    "    \"\"\"\n",
    "    print(f\"Transformation type: {transformation_type}\")\n",
    "    print(f\"Original query: {query}\")\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    if transformation_type == \"rewrite\":\n",
    "        # Query rewriting\n",
    "        transformed_query = rewrite_query(query)\n",
    "        print(f\"Rewritten query: {transformed_query}\")\n",
    "        \n",
    "        # Create embedding for transformed query\n",
    "        query_embedding = create_embeddings(transformed_query)\n",
    "        \n",
    "        # Search with rewritten query\n",
    "        results = vector_store.similarity_search(query_embedding, k=top_k)\n",
    "        \n",
    "    elif transformation_type == \"step_back\":\n",
    "        # Step-back prompting\n",
    "        transformed_query = generate_step_back_query(query)\n",
    "        print(f\"Step-back query: {transformed_query}\")\n",
    "        \n",
    "        # Create embedding for transformed query\n",
    "        query_embedding = create_embeddings(transformed_query)\n",
    "        \n",
    "        # Search with step-back query\n",
    "        results = vector_store.similarity_search(query_embedding, k=top_k)\n",
    "        \n",
    "    elif transformation_type == \"decompose\":\n",
    "        # Sub-query decomposition\n",
    "        sub_queries = decompose_query(query)\n",
    "        print(\"Decomposed into sub-queries:\")\n",
    "        for i, sub_q in enumerate(sub_queries, 1):\n",
    "            print(f\"{i}. {sub_q}\")\n",
    "        \n",
    "        # Create embeddings for all sub-queries\n",
    "        sub_query_embeddings = create_embeddings(sub_queries)\n",
    "        \n",
    "        # Search with each sub-query and combine results\n",
    "        all_results = []\n",
    "        for i, embedding in enumerate(sub_query_embeddings):\n",
    "            sub_results = vector_store.similarity_search(embedding, k=2)  # Get fewer results per sub-query\n",
    "            all_results.extend(sub_results)\n",
    "        \n",
    "        # Remove duplicates (keep highest similarity score)\n",
    "        seen_texts = {}\n",
    "        for result in all_results:\n",
    "            text = result[\"text\"]\n",
    "            if text not in seen_texts or result[\"similarity\"] > seen_texts[text][\"similarity\"]:\n",
    "                seen_texts[text] = result\n",
    "        \n",
    "        # Sort by similarity and take top_k\n",
    "        results = sorted(seen_texts.values(), key=lambda x: x[\"similarity\"], reverse=True)[:top_k]\n",
    "        \n",
    "    else:\n",
    "        # Regular search without transformation\n",
    "        query_embedding = create_embeddings(query)\n",
    "        results = vector_store.similarity_search(query_embedding, k=top_k)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating a Response with Transformed Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(query, context, model=\"meta-llama/Llama-3.2-3B-Instruct\"):\n",
    "    \"\"\"\n",
    "    Generates a response based on the query and retrieved context.\n",
    "    \n",
    "    Args:\n",
    "        query (str): User query\n",
    "        context (str): Retrieved context\n",
    "        model (str): The model to use for response generation\n",
    "        \n",
    "    Returns:\n",
    "        str: Generated response\n",
    "    \"\"\"\n",
    "    # Define the system prompt to guide the AI assistant's behavior\n",
    "    system_prompt = \"You are a helpful AI assistant. Answer the user's question based only on the provided context. If you cannot find the answer in the context, state that you don't have enough information.\"\n",
    "    \n",
    "    # Define the user prompt with the context and query\n",
    "    user_prompt = f\"\"\"\n",
    "        Context:\n",
    "        {context}\n",
    "\n",
    "        Question: {query}\n",
    "\n",
    "        Please provide a comprehensive answer based only on the context above.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Generate the response using the specified model\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        temperature=0,  # Low temperature for deterministic output\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # Return the generated response, stripping any leading/trailing whitespace\n",
    "    return response.choices[0].message.content.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the Complete RAG Pipeline with Query Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_with_query_transformation(pdf_path, query, transformation_type=None):\n",
    "    \"\"\"\n",
    "    Run complete RAG pipeline with optional query transformation.\n",
    "    \n",
    "    Args:\n",
    "        pdf_path (str): Path to PDF document\n",
    "        query (str): User query\n",
    "        transformation_type (str): Type of transformation (None, 'rewrite', 'step_back', or 'decompose')\n",
    "        \n",
    "    Returns:\n",
    "        Dict: Results including query, transformed query, context, and response\n",
    "    \"\"\"\n",
    "    # Process the document to create a vector store\n",
    "    vector_store = process_document(pdf_path)\n",
    "    \n",
    "    # Apply query transformation and search\n",
    "    if transformation_type:\n",
    "        # Perform search with transformed query\n",
    "        results = transformed_search(query, vector_store, transformation_type)\n",
    "    else:\n",
    "        # Perform regular search without transformation\n",
    "        query_embedding = create_embeddings(query)\n",
    "        results = vector_store.similarity_search(query_embedding, k=3)\n",
    "    \n",
    "    # Combine context from search results\n",
    "    context = \"\\n\\n\".join([f\"PASSAGE {i+1}:\\n{result['text']}\" for i, result in enumerate(results)])\n",
    "    \n",
    "    # Generate response based on the query and combined context\n",
    "    response = generate_response(query, context)\n",
    "    \n",
    "    # Return the results including original query, transformation type, context, and response\n",
    "    return {\n",
    "        \"original_query\": query,\n",
    "        \"transformation_type\": transformation_type,\n",
    "        \"context\": context,\n",
    "        \"response\": response\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Transformation Techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compare_responses(results, reference_answer, model=\"meta-llama/Llama-3.2-3B-Instruct\"):\n",
    "    \"\"\"\n",
    "    Compare responses from different query transformation techniques.\n",
    "    \n",
    "    Args:\n",
    "        results (Dict): Results from different transformation techniques\n",
    "        reference_answer (str): Reference answer for comparison\n",
    "        model (str): Model for evaluation\n",
    "    \"\"\"\n",
    "    # Define the system prompt to guide the AI assistant's behavior\n",
    "    system_prompt = \"\"\"You are an expert evaluator of RAG systems. \n",
    "    Your task is to compare different responses generated using various query transformation techniques \n",
    "    and determine which technique produced the best response compared to the reference answer.\"\"\"\n",
    "    \n",
    "    # Prepare the comparison text with the reference answer and responses from each technique\n",
    "    comparison_text = f\"\"\"Reference Answer: {reference_answer}\\n\\n\"\"\"\n",
    "    \n",
    "    for technique, result in results.items():\n",
    "        comparison_text += f\"{technique.capitalize()} Query Response:\\n{result['response']}\\n\\n\"\n",
    "    \n",
    "    # Define the user prompt with the comparison text\n",
    "    user_prompt = f\"\"\"\n",
    "    {comparison_text}\n",
    "    \n",
    "    Compare the responses generated by different query transformation techniques to the reference answer.\n",
    "    \n",
    "    For each technique (original, rewrite, step_back, decompose):\n",
    "    1. Score the response from 1-10 based on accuracy, completeness, and relevance\n",
    "    2. Identify strengths and weaknesses\n",
    "    \n",
    "    Then rank the techniques from best to worst and explain which technique performed best overall and why.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Generate the evaluation response using the specified model\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        temperature=0,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # Print the evaluation results\n",
    "    print(\"\\n===== EVALUATION RESULTS =====\")\n",
    "    print(response.choices[0].message.content)\n",
    "    print(\"=============================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_transformations(pdf_path, query, reference_answer=None):\n",
    "    \"\"\"\n",
    "    Evaluate different transformation techniques for the same query.\n",
    "    \n",
    "    Args:\n",
    "        pdf_path (str): Path to PDF document\n",
    "        query (str): Query to evaluate\n",
    "        reference_answer (str): Optional reference answer for comparison\n",
    "        \n",
    "    Returns:\n",
    "        Dict: Evaluation results\n",
    "    \"\"\"\n",
    "    # Define the transformation techniques to evaluate\n",
    "    transformation_types = [None, \"rewrite\", \"step_back\", \"decompose\"]\n",
    "    results = {}\n",
    "    \n",
    "    # Run RAG with each transformation technique\n",
    "    for transformation_type in transformation_types:\n",
    "        type_name = transformation_type if transformation_type else \"original\"\n",
    "        print(f\"\\n===== Running RAG with {type_name} query =====\")\n",
    "        \n",
    "        # Get the result for the current transformation type\n",
    "        result = rag_with_query_transformation(pdf_path, query, transformation_type)\n",
    "        results[type_name] = result\n",
    "        \n",
    "        # Print the response for the current transformation type\n",
    "        print(f\"Response with {type_name} query:\")\n",
    "        print(result[\"response\"])\n",
    "        print(\"=\" * 50)\n",
    "    \n",
    "    # Compare results if a reference answer is provided\n",
    "    if reference_answer:\n",
    "        compare_responses(results, reference_answer)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of Query Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Running RAG with original query =====\n",
      "Extracting text from PDF...\n",
      "Chunking text...\n",
      "Created 42 text chunks\n",
      "Creating embeddings for chunks...\n",
      "Added 42 chunks to the vector store\n",
      "Response with original query:\n",
      "Explainable AI (XAI) refers to techniques used to make AI decisions more understandable and transparent. The primary goal of XAI is to enable users to assess the fairness and accuracy of AI systems. This is achieved by providing insights into how AI models make decisions, thereby enhancing trust and accountability.\n",
      "\n",
      "XAI is considered important for several reasons:\n",
      "\n",
      "1. **Building trust in AI**: Transparency and explainability are key to building trust in AI systems. By making AI decisions more understandable, users can assess the reliability and fairness of these systems.\n",
      "\n",
      "2. **Addressing potential harms**: Establishing accountability and responsibility for AI systems is essential for addressing potential harms and ensuring ethical behavior. XAI helps in this regard by providing insights into AI decision-making processes.\n",
      "\n",
      "3. **Ensuring data protection**: AI systems often rely on large amounts of data, raising concerns about privacy and data protection. XAI techniques can help ensure responsible data handling and compliance with data protection regulations.\n",
      "\n",
      "4. **Improving accountability**: XAI techniques are being developed to provide insights into how AI models make decisions, which can help in establishing clear guidelines and ethical frameworks for AI development and deployment.\n",
      "\n",
      "5. **Enhancing trust and accountability**: By making AI systems more transparent and understandable, XAI techniques can enhance trust and accountability in AI decision-making processes.\n",
      "\n",
      "In summary, Explainable AI is a crucial aspect of building trust in AI systems, addressing potential harms, ensuring data protection, and improving accountability. Its development and application are essential for the responsible and ethical use of AI.\n",
      "==================================================\n",
      "\n",
      "===== Running RAG with rewrite query =====\n",
      "Extracting text from PDF...\n",
      "Chunking text...\n",
      "Created 42 text chunks\n",
      "Creating embeddings for chunks...\n",
      "Added 42 chunks to the vector store\n",
      "Transformation type: rewrite\n",
      "Original query: What is 'Explainable AI' and why is it considered important?\n",
      "Rewritten query: Here's a rewritten query:\n",
      "\n",
      "\"Explainable AI (XAI) definition, key concepts, and importance in machine learning, with a focus on its applications in high-stakes decision-making, transparency, and accountability, and how it addresses the limitations of black box models.\"\n",
      "\n",
      "This rewritten query is more specific and detailed because it:\n",
      "\n",
      "1. Includes the term \"Explainable AI\" (XAI) to ensure the search results are focused on the specific topic.\n",
      "2. Specifies the definition of XAI, which will help retrieve relevant information from academic and technical sources.\n",
      "3. Highlights the key concepts of XAI, such as its applications in high-stakes decision-making, transparency, and accountability.\n",
      "4. Emphasizes the limitations of black box models, which is a common concern in the field of XAI.\n",
      "5. Uses relevant terms like \"machine learning\" and \"high-stakes decision-making\" to narrow down the search results to the most relevant and accurate information.\n",
      "\n",
      "By incorporating these specific terms and concepts, the rewritten query is more likely to retrieve accurate and relevant information about Explainable AI.\n",
      "Response with rewrite query:\n",
      "Explainable AI (XAI) refers to techniques that aim to make AI decisions more understandable, enabling users to assess their fairness and accuracy. The primary goal of XAI is to provide insights into the decision-making processes of AI systems, thereby enhancing transparency and trust in these systems.\n",
      "\n",
      "XAI is considered important for several reasons:\n",
      "\n",
      "1. **Building trust**: By making AI decisions more understandable, XAI helps build trust in AI systems, which is essential for their widespread adoption.\n",
      "2. **Assessing fairness and accuracy**: XAI enables users to evaluate the fairness and accuracy of AI decisions, which is crucial for ensuring that AI systems are reliable and unbiased.\n",
      "3. **Addressing accountability**: XAI helps establish accountability for AI systems, which is essential for addressing potential harms and ensuring ethical behavior.\n",
      "4. **Improving explainability**: XAI research focuses on developing methods for explaining AI decisions, which can lead to more interpretable and transparent AI systems.\n",
      "\n",
      "In summary, Explainable AI is a crucial aspect of building trust, ensuring fairness and accuracy, and promoting accountability in AI systems.\n",
      "==================================================\n",
      "\n",
      "===== Running RAG with step_back query =====\n",
      "Extracting text from PDF...\n",
      "Chunking text...\n",
      "Created 42 text chunks\n",
      "Creating embeddings for chunks...\n",
      "Added 42 chunks to the vector store\n",
      "Transformation type: step_back\n",
      "Original query: What is 'Explainable AI' and why is it considered important?\n",
      "Step-back query: Here's a broader, more general version of the query:\n",
      "\n",
      "\"Background information on the concept and significance of Explainable AI in the context of artificial intelligence and its applications.\"\n",
      "\n",
      "This step-back query can help retrieve relevant background information on Explainable AI, including its definition, history, key concepts, and importance in the broader field of artificial intelligence.\n",
      "Response with step_back query:\n",
      "Based on the provided context, Explainable AI (XAI) refers to techniques that aim to make AI decisions more understandable, enabling users to assess their fairness and accuracy. The primary goal of XAI is to provide insights into the decision-making processes of AI systems, thereby enhancing trust and accountability.\n",
      "\n",
      "XAI is considered important for several reasons:\n",
      "\n",
      "1. **Transparency and Explainability**: XAI helps make AI systems more transparent and understandable, allowing users to assess their reliability and fairness.\n",
      "2. **Addressing Potential Harms**: By establishing accountability and responsibility for AI systems, XAI can help mitigate potential harms and ensure ethical behavior.\n",
      "3. **Building Trust in AI**: XAI is crucial for building trust in AI, as it enables users to understand the reasoning behind AI decisions and make informed decisions.\n",
      "4. **Improving Accountability**: XAI helps to establish clear roles and responsibilities for developers, deployers, and users of AI systems, ensuring that those responsible for AI decisions are accountable for their actions.\n",
      "\n",
      "Overall, Explainable AI is a critical aspect of building trust in AI, ensuring accountability, and mitigating potential harms.\n",
      "==================================================\n",
      "\n",
      "===== Running RAG with decompose query =====\n",
      "Extracting text from PDF...\n",
      "Chunking text...\n",
      "Created 42 text chunks\n",
      "Creating embeddings for chunks...\n",
      "Added 42 chunks to the vector store\n",
      "Transformation type: decompose\n",
      "Original query: What is 'Explainable AI' and why is it considered important?\n",
      "Decomposed into sub-queries:\n",
      "1. What does the term 'Explainable AI' refer to, and how is it different from traditional machine learning models?\n",
      "2. What are the primary goals and objectives of Explainable AI, and how do they align with broader societal needs?\n",
      "3. What are the key characteristics and features of Explainable AI systems, and how do they enable transparency and trustworthiness?\n",
      "4. What are the potential benefits and applications of Explainable AI, and how might it impact various industries and fields of study?\n",
      "Response with decompose query:\n",
      "Explainable AI (XAI) is a set of techniques aimed at making AI decisions more understandable, enabling users to assess their fairness and accuracy. The primary goal of XAI is to provide insights into the decision-making processes of AI systems, thereby enhancing trust in these systems.\n",
      "\n",
      "XAI is considered important for several reasons:\n",
      "\n",
      "1. **Transparency and Explainability**: XAI helps make AI systems more transparent and understandable, allowing users to assess their reliability and fairness.\n",
      "2. **Accountability and Responsibility**: By providing insights into AI decision-making processes, XAI enables accountability and responsibility for AI systems, which is essential for addressing potential harms and ensuring ethical behavior.\n",
      "3. **Building Trust**: XAI is crucial for building trust in AI systems, as it empowers users to make informed decisions about their interactions with AI.\n",
      "4. **Robustness and Reliability**: XAI contributes to ensuring that AI systems are robust and reliable, which is essential for building trust.\n",
      "\n",
      "In summary, Explainable AI is a vital aspect of building trust in AI systems, as it enables users to understand and assess the fairness and accuracy of AI decisions, promotes accountability and responsibility, and contributes to the overall robustness and reliability of AI systems.\n",
      "==================================================\n",
      "\n",
      "===== EVALUATION RESULTS =====\n",
      "After comparing the responses generated by different query transformation techniques to the reference answer, I have scored each response based on accuracy, completeness, and relevance. Here are the results:\n",
      "\n",
      "**Original Query Response**\n",
      "\n",
      "* Score: 8/10\n",
      "* Strengths:\n",
      "\t+ Maintains the original tone and structure of the reference answer\n",
      "\t+ Covers all the key points mentioned in the reference answer\n",
      "\t+ Provides a clear and concise explanation of Explainable AI (XAI)\n",
      "* Weaknesses:\n",
      "\t+ Some sentences are wordy and could be rephrased for better clarity\n",
      "\t+ The transition between points could be smoother\n",
      "\n",
      "**Rewrite Query Response**\n",
      "\n",
      "* Score: 7.5/10\n",
      "* Strengths:\n",
      "\t+ Simplifies the language and makes the response more accessible to a wider audience\n",
      "\t+ Reduces the number of sentences and makes the response more concise\n",
      "\t+ Still covers all the key points mentioned in the reference answer\n",
      "* Weaknesses:\n",
      "\t+ Loses some of the nuance and detail of the original response\n",
      "\t+ Some sentences are still wordy and could be rephrased for better clarity\n",
      "\n",
      "**Step_back Query Response**\n",
      "\n",
      "* Score: 8.5/10\n",
      "* Strengths:\n",
      "\t+ Provides a clear and concise summary of the reference answer\n",
      "\t+ Uses more active and engaging language\n",
      "\t+ Covers all the key points mentioned in the reference answer\n",
      "* Weaknesses:\n",
      "\t+ Some sentences are still wordy and could be rephrased for better clarity\n",
      "\t+ The transition between points could be smoother\n",
      "\n",
      "**Decompose Query Response**\n",
      "\n",
      "* Score: 9/10\n",
      "* Strengths:\n",
      "\t+ Provides a clear and concise explanation of Explainable AI (XAI)\n",
      "\t+ Uses more active and engaging language\n",
      "\t+ Covers all the key points mentioned in the reference answer\n",
      "\t+ Breaks down the information into smaller, more manageable chunks\n",
      "* Weaknesses:\n",
      "\t+ None notable\n",
      "\n",
      "Based on the scores, the **Decompose Query Response** performed the best overall, with a score of 9/10. This response provided a clear and concise explanation of Explainable AI (XAI), covered all the key points mentioned in the reference answer, and used more active and engaging language.\n",
      "\n",
      "The **Step_back Query Response** came in second, with a score of 8.5/10. This response provided a clear and concise summary of the reference answer, used more active and engaging language, and covered all the key points mentioned in the reference answer.\n",
      "\n",
      "The **Original Query Response** and **Rewrite Query Response** both scored 8/10 and 7.5/10, respectively. While they both provided a good explanation of Explainable AI (XAI), they had some weaknesses, such as wordy sentences and a less smooth transition between points.\n",
      "\n",
      "The **Rewrite Query Response** performed the worst, with a score of 7.5/10. This response simplified the language and made the response more concise, but lost some of the nuance and detail of the original response.\n",
      "\n",
      "Overall, the Decompose Query Response performed the best overall due to its clear and concise explanation of Explainable AI (XAI), its use of more active and engaging language, and its ability to break down the information into smaller, more manageable chunks.\n",
      "=============================\n"
     ]
    }
   ],
   "source": [
    "# Load the validation data from a JSON file\n",
    "with open('data/val.json') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Extract the first query from the validation data\n",
    "query = data[0]['question']\n",
    "\n",
    "# Extract the reference answer from the validation data\n",
    "reference_answer = data[0]['ideal_answer']\n",
    "\n",
    "# pdf_path\n",
    "pdf_path = \"data/AI_information.pdf\"\n",
    "\n",
    "# Run evaluation\n",
    "evaluation_results = evaluate_transformations(pdf_path, query, reference_answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
